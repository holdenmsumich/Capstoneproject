{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dfbdb3e0-d1f6-414f-b682-d8a7fb69c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenizer(description, stop_words, normalization):\n",
    "    \n",
    "    if normalization == 'lemmatize':\n",
    "        # tokenize and lemmatize text\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in word_tokenize(description)]\n",
    "        \n",
    "    elif normalization == 'stem':\n",
    "        # tokenize and stem text\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(w) for w in word_tokenize(description)]\n",
    "    \n",
    "   # remove tokens length of 2 or below and make all lowercase and remove stop words\n",
    "    tokens = [w.lower() for w in tokens if (w.lower() not in stop_words) and (len(w) > 2) and(w.isalpha())]\n",
    "    \n",
    "    return tokens    \n",
    "    \n",
    "def process_query(query, normalization):\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    return tokenizer(query, stop_words, normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "511d07fb-bbaf-494c-b73f-16ab8c017926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_n_rank_docs(inverted_index, query, normalization, max_docs=-1):\n",
    "    ret_docs = {}\n",
    "    \n",
    "    counts = {}\n",
    "    query = process_query(query, normalization\n",
    "                         )\n",
    "    for word in query:\n",
    "        try:\n",
    "            docs = inverted_index.get(word)\n",
    "            for k, v in docs.items():\n",
    "                if k in counts:\n",
    "                    counts[k] += v\n",
    "                else:\n",
    "                    counts[k] = v\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        break\n",
    "    counts = sorted(counts.items(), key=lambda x: (x[1], -int(x[0][1:])), reverse=True)\n",
    "    if max_docs > -1:\n",
    "        ret_docs[' '.join(query)] = [x[0] for x in counts][:max_docs]\n",
    "    else:\n",
    "        ret_docs[' '.join(query)] = [x[0] for x in counts]\n",
    "        \n",
    "    return ret_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c22c5b9e-47cb-4d3d-bc50-62ddf9a51f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT INVERTED_INDEXES\n",
    "with open(r'assets/inverted_index_stem.json') as f:\n",
    "    inverted_index_stem = json.load(f)\n",
    "    \n",
    "with open(r'assets/inverted_index_lem.json') as f:\n",
    "    inverted_index_lem = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "df483fd6-d764-4ed8-9f51-fd23de8dd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "naics_titles = pd.read_excel('assets/6-digit_2017_Codes.xlsx')\n",
    "naics_titles['naics'] = naics_titles['naics'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef167736-a424-4410-b158-f130333504a1",
   "metadata": {},
   "source": [
    "## Stemmed Word Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0eab3f30-7c1d-4bf4-9b95-d8a3c95a0387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naics</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321999</td>\n",
       "      <td>All Other Miscellaneous Wood Product Manufactu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453998</td>\n",
       "      <td>All Other Miscellaneous Store Retailers (excep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423220</td>\n",
       "      <td>Home Furnishing Merchant Wholesalers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>454390</td>\n",
       "      <td>Other Direct Selling Establishments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333111</td>\n",
       "      <td>Farm Machinery and Equipment Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>321920</td>\n",
       "      <td>Wood Container and Pallet Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>423390</td>\n",
       "      <td>Other Construction Material Merchant Wholesalers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>623990</td>\n",
       "      <td>Other Residential Care Facilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>236117</td>\n",
       "      <td>New Housing For-Sale Builders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>442299</td>\n",
       "      <td>All Other Home Furnishings Stores</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    naics                                              title\n",
       "0  321999  All Other Miscellaneous Wood Product Manufactu...\n",
       "1  453998  All Other Miscellaneous Store Retailers (excep...\n",
       "2  423220              Home Furnishing Merchant Wholesalers \n",
       "3  454390               Other Direct Selling Establishments \n",
       "4  333111        Farm Machinery and Equipment Manufacturing \n",
       "5  321920            Wood Container and Pallet Manufacturing\n",
       "6  423390  Other Construction Material Merchant Wholesalers \n",
       "7  623990                 Other Residential Care Facilities \n",
       "8  236117                     New Housing For-Sale Builders \n",
       "9  442299                 All Other Home Furnishings Stores "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_df = pd.DataFrame(retrieve_n_rank_docs(inverted_index_lem, 'Home improvement store', 'stem'))\n",
    "stem_df.columns.values[0] = 'naics'\n",
    "stem_df = stem_df.merge(naics_titles, on='naics', how='outer')\n",
    "stem_df[['naics', 'title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "31f31245-6d60-4eb7-8f26-07f37946c33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naics</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>453998</td>\n",
       "      <td>All Other Miscellaneous Store Retailers (excep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>511199</td>\n",
       "      <td>All Other Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>611610</td>\n",
       "      <td>Fine Arts Schools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512230</td>\n",
       "      <td>Music Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>511120</td>\n",
       "      <td>Periodical Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>511130</td>\n",
       "      <td>Book Publishers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>621340</td>\n",
       "      <td>Offices of Physical, Occupational and Speech T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>453310</td>\n",
       "      <td>Used Merchandise Stores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>512290</td>\n",
       "      <td>Other Sound Recording Industries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>511140</td>\n",
       "      <td>Directory and Mailing List Publishers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    naics                                              title\n",
       "0  453998  All Other Miscellaneous Store Retailers (excep...\n",
       "1  511199                              All Other Publishers \n",
       "2  611610                                 Fine Arts Schools \n",
       "3  512230                                   Music Publishers\n",
       "4  511120                             Periodical Publishers \n",
       "5  511130                                   Book Publishers \n",
       "6  621340  Offices of Physical, Occupational and Speech T...\n",
       "7  453310                           Used Merchandise Stores \n",
       "8  512290                   Other Sound Recording Industries\n",
       "9  511140             Directory and Mailing List Publishers "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_df = pd.DataFrame(retrieve_n_rank_docs(inverted_index_lem, 'musical instrument store', 'stem'))\n",
    "stem_df.columns.values[0] = 'naics'\n",
    "stem_df = stem_df.merge(naics_titles, on='naics', how='outer')\n",
    "stem_df[['naics', 'title']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42632048-7894-4710-886f-060d30e306c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lemmatized Word Count Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "552b9b13-788c-485f-b80b-3bacc257fcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naics</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321999</td>\n",
       "      <td>All Other Miscellaneous Wood Product Manufactu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>453998</td>\n",
       "      <td>All Other Miscellaneous Store Retailers (excep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423220</td>\n",
       "      <td>Home Furnishing Merchant Wholesalers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>454390</td>\n",
       "      <td>Other Direct Selling Establishments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333111</td>\n",
       "      <td>Farm Machinery and Equipment Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>321920</td>\n",
       "      <td>Wood Container and Pallet Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>423390</td>\n",
       "      <td>Other Construction Material Merchant Wholesalers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>623990</td>\n",
       "      <td>Other Residential Care Facilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>236117</td>\n",
       "      <td>New Housing For-Sale Builders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>442299</td>\n",
       "      <td>All Other Home Furnishings Stores</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    naics                                              title\n",
       "0  321999  All Other Miscellaneous Wood Product Manufactu...\n",
       "1  453998  All Other Miscellaneous Store Retailers (excep...\n",
       "2  423220              Home Furnishing Merchant Wholesalers \n",
       "3  454390               Other Direct Selling Establishments \n",
       "4  333111        Farm Machinery and Equipment Manufacturing \n",
       "5  321920            Wood Container and Pallet Manufacturing\n",
       "6  423390  Other Construction Material Merchant Wholesalers \n",
       "7  623990                 Other Residential Care Facilities \n",
       "8  236117                     New Housing For-Sale Builders \n",
       "9  442299                 All Other Home Furnishings Stores "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_df = pd.DataFrame(retrieve_n_rank_docs(inverted_index_lem, 'Home improvement store', 'lemmatize'))\n",
    "lem_df.columns.values[0] = 'naics'\n",
    "lem_df = lem_df.merge(naics_titles, on='naics', how='outer')\n",
    "lem_df[['naics', 'title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6214704a-1e36-4a7a-83be-550507ccfcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naics</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>711510</td>\n",
       "      <td>Independent Artists, Writers, and Performers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>711130</td>\n",
       "      <td>Musical Groups and Artists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>711310</td>\n",
       "      <td>Promoters of Performing Arts, Sports, and Simi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711320</td>\n",
       "      <td>Promoters of Performing Arts, Sports, and Simi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>711110</td>\n",
       "      <td>Theater Companies and Dinner Theaters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>711219</td>\n",
       "      <td>Other Spectator Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>339992</td>\n",
       "      <td>Musical Instrument Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>711410</td>\n",
       "      <td>Agents and Managers for Artists, Athletes, Ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>711211</td>\n",
       "      <td>Sports Teams and Clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>339999</td>\n",
       "      <td>All Other Miscellaneous Manufacturing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    naics                                              title\n",
       "0  711510      Independent Artists, Writers, and Performers \n",
       "1  711130                        Musical Groups and Artists \n",
       "2  711310  Promoters of Performing Arts, Sports, and Simi...\n",
       "3  711320  Promoters of Performing Arts, Sports, and Simi...\n",
       "4  711110             Theater Companies and Dinner Theaters \n",
       "5  711219                            Other Spectator Sports \n",
       "6  339992                  Musical Instrument Manufacturing \n",
       "7  711410  Agents and Managers for Artists, Athletes, Ent...\n",
       "8  711211                            Sports Teams and Clubs \n",
       "9  339999             All Other Miscellaneous Manufacturing "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_df = pd.DataFrame(retrieve_n_rank_docs(inverted_index_lem, 'musical instrument store', 'lemmatize'))\n",
    "lem_df.columns.values[0] = 'naics'\n",
    "lem_df = lem_df.merge(naics_titles, on='naics', how='outer')\n",
    "lem_df[['naics', 'title']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc4b31-6dc1-4819-ac72-e7db984bac4d",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6dd14a1a-4f06-4953-81a5-dd50e8a789f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_judgments = pd.read_pickle('assets/relevant_naics_df.pkl')\n",
    "relevance_judgments = dict(zip(relevance_judgments['query'], relevance_judgments['relevant_naics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b312da1-a8f6-4740-b273-019365621ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(normalization='lemmatize'):\n",
    "    for query in relevance_judgments:\n",
    "        ret_docs = retrieve_n_rank_docs(inverted_index_stem, query, normalization, max_docs=max_docs)\n",
    "        print(ret_docs)\n",
    "        query_docs = relevance_judgments\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac44df-6224-49e8-a9c9-339b7446c5f6",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "445551e4-67c3-4780-a8ac-3ea9bb07b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT INVERTED_INDEX\n",
    "with open(r'assets/inverted_index_stem.json') as f:\n",
    "    inverted_index_stem = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "24bb87b6-1772-4e1a-84bd-32c9a63d0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_docs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2445edbc-37df-42c9-a4db-e6aa7fe5680b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_test_data(normalization='lemmatize'):\n",
    "    ret_docs_dic_stem = {}\n",
    "    queries_dic_stem = {}\n",
    "\n",
    "    for query, value in relevance_judgments.items():\n",
    "        processed_query = process_query(query, normalization)\n",
    "        ret_docs = retrieve_n_rank_docs(inverted_index_stem, processed_query, normalization, max_docs=max_docs)\n",
    "        ret_docs_dic_stem[processed_query] = value\n",
    "        return ret_docs_dic_stem\n",
    "        query_docs = relevance_judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6709c5fb-81da-467a-aa5f-9ce900462678",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [133]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcreate_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [132]\u001b[0m, in \u001b[0;36mcreate_test_data\u001b[1;34m(normalization)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query, value \u001b[38;5;129;01min\u001b[39;00m relevance_judgments\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m     processed_query \u001b[38;5;241m=\u001b[39m process_query(query, normalization)\n\u001b[1;32m----> 7\u001b[0m     ret_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_n_rank_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverted_index_stem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     ret_docs_dic_stem[processed_query] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_docs_dic_stem\n",
      "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36mretrieve_n_rank_docs\u001b[1;34m(inverted_index, query, normalization, max_docs)\u001b[0m\n\u001b[0;32m      2\u001b[0m ret_docs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m counts \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalization\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36mprocess_query\u001b[1;34m(query, normalization)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_query\u001b[39m(query, normalization):\n\u001b[0;32m     28\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalization\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36mtokenizer\u001b[1;34m(description, stop_words, normalization)\u001b[0m\n\u001b[0;32m     16\u001b[0m  \u001b[38;5;28;01melif\u001b[39;00m normalization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     17\u001b[0m      \u001b[38;5;66;03m# tokenize and stem text\u001b[39;00m\n\u001b[0;32m     18\u001b[0m      stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 19\u001b[0m      tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# remove tokens length of 2 or below and make all lowercase and remove stop words\u001b[39;00m\n\u001b[0;32m     22\u001b[0m  tokens \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m (w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m(w\u001b[38;5;241m.\u001b[39misalpha())]\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\PycharmProjects\\CapstonEproject\\venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "create_test_data(normalization='stem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57385ec7-7f51-4361-8927-74b78c3e470d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
