{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "505b49c1-51ea-4054-81eb-313e098146ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a352da46-1757-4c7a-b37e-dc88f688e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenizer(description, stop_words, normalization):\n",
    "    \n",
    "    if normalization == 'lemmatize':\n",
    "        # tokenize and lemmatize text\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in word_tokenize(description)]\n",
    "        \n",
    "    elif normalization == 'stem':\n",
    "        # tokenize and stem text\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(w) for w in word_tokenize(description)]\n",
    "    \n",
    "   # remove tokens length of 2 or below and make all lowercase and remove stop words\n",
    "    tokens = [w.lower() for w in tokens if (w.lower() not in stop_words) and (len(w) > 2) and(w.isalpha())]\n",
    "    \n",
    "    return tokens    \n",
    "    \n",
    "def process_query(query, normalization):\n",
    "    \n",
    "    return tokenizer(query, stop_words, normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ec43a45-bfe8-4c34-a561-90d5f42345e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code example taken from https://towardsdatascience.com/build-a-text-recommendation-system-with-python-e8b95d9f251c\n",
    "def retrieve_top_n(m, max_docs):\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # from highest idx to smallest score \\\n",
    "    mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:max_docs]  \n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66c4d3fb-5e2e-4c72-b295-b9e42e4437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfRecommenderSystem:\n",
    "    def __init__(self, docs, num_concepts=100, min_df=1, alpha=1.0, beta=0.75, gamma=0.15):\n",
    "        self.alpha, self.beta, self.gamma = alpha, beta, gamma\n",
    "        \n",
    "        # create a doc-term matrix out of our doc collection\n",
    "        self.vec = TfidfVectorizer()\n",
    "        doc_term_mat = self.vec.fit_transform([\" \".join(docs[doc_id]) for doc_id in docs])\n",
    "        result = doc_term_mat\n",
    "        \n",
    "        self.q_vecs = {}\n",
    "        \n",
    "        self.doc_vecs = result # document vectors in a matrix\n",
    "        \n",
    "    def retrieve_docs(self, query, max_docs=15):\n",
    "        query = process_query(query, 'lemmatize')\n",
    "        \n",
    "        if query not in self.q_vecs:\n",
    "            q_vec = self.vec.transform([query])\n",
    "            self.q_vecs[query] = q_vec\n",
    "        \n",
    "        ret_docs = {}\n",
    "        \n",
    "        mat = cosine_similarity(self.q_vecs[query], self.doc_vecs)\n",
    "        best_index = retrieve_top_n(mat, topk=max_docs)\n",
    "        \n",
    "        return best_index\n",
    "    \n",
    "    def gather_feedback(self, query, max_docs=15, feedback=None):\n",
    "        \"\"\"\n",
    "        This function models the interactive relevance feedback loop\n",
    "        \"\"\"\n",
    "        query = process_query(query, 'lemmatize')\n",
    "        # Step 2: Retrieve the required number of docs in reponse to the queries\n",
    "        ret_docs = self.retrieve_docs(query, max_docs=max_docs)\n",
    "        \n",
    "        \n",
    "        # display docs to user\n",
    "        # receive feedback from user\n",
    "\n",
    "        # Step 3: Obtain feedback from the user in the form of precisions at each rank\n",
    "        user_feedback = feedback\n",
    "        # map index to user feedback\n",
    "        idx_dic = {}\n",
    "        \n",
    "        for i, doc in enumerate(ret_docs):\n",
    "            try:\n",
    "                idx_dic[doc] = user_feedback[i]\n",
    "            except:\n",
    "                idx_dic[doc] = 0\n",
    "\n",
    "        self.q_vecs[query] = np.dot(self.alpha, self.q_vecs[query])\n",
    "        for key, value in idx_dic.items():\n",
    "            if value == 1:\n",
    "                self.q_vecs[query] += np.dot(self.beta, self.doc_vecs[key])\n",
    "            else:\n",
    "                self.q_vecs[query] -= np.dot(self.gamma, self.doc_vecs[key])\n",
    "        self.q_vecs[query][self.q_vecs[query] < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b803483-746f-4309-9d39-6ce3e2328b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r'assets/processed_df.pkl')\n",
    "docs = dict(zip(df['naics'], df['lemmatized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c31a287-1569-48e5-8612-6d4fd1380828",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfRecommenderSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f648978-c31c-4aef-86bb-7c89dcb67c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
